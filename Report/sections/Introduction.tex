\newpage
\section{Introduction}
\setcounter{subsection}{0}

\examplefigure[0.5]{images/treap.png}{This figure gives an example of treap data structure. The black number is the key, whereas the red number is the heap priority}{fig:treap}

\noindent A treap, also known as a Cartesian tree, is a data structure that combines the features of a binary search tree (BST) and a binary heap. The combination allows the tree to have an expected logarithmic depth of its number of nodes so that, with high probability, the tree is balanced. It also performs less than two rotations for updating its contents \cite{seidel1996randomized}, which makes it more efficient compared with the deterministic counterparts. Its randomness property makes it efficient to maintain a dynamic set of ordered elements while allowing for fast search, insertion, and deletion operations. During these operations, treap does not guarantee its topology to be consistently balanced. Hence, one must be careful when using this data structure if a specific time-bound is required. Treaps are particularly useful in cases where the data set changes over time and we want to maintain the elements in a sorted manner.\\

In a treap, each node has two attributes: a key and a priority. The key is used to maintain the binary search tree property, that is to ensure for each node, all the keys in its left subtree are smaller and all the keys in its right subtree are larger. As the keys hold an essential role in searching the data, we keep the keys unique. The priority, on the other hand, is used to maintain the heap property which controls the balance. For this purpose, we may allow duplicates up to some limits. For maintaining the property, we can use max-heap or min-heap property. In this project, we used max-heap property as demonstrated in Figure \ref*{fig:treap} -- The priority of a node is at least the same as its children's priority.\\

The expected height of the treap is $O(\log n)$, which means that the search, insertion, and deletion operations are efficient in practice. However, there is no guarantee that a treap will always achieve logarithmic height because of the randomized priority. The worst-case complexity of treap operations can be $O(n)$, which is idential with a linked-list. This bad scenario occurs when the priorities lead to a highly unbalanced tree. Despite having some drawbacks, treaps are often used because of their simplicity and good average-case performance.\\

\subsection{Treap Operations}
Treap operations are just the same as operations used in binary search tree. The basic operations includes build, insert, delete, search, split, and merge. There are also other operations such as union and intersect. But we will only discuss the six previously mentioned operations as this project can handle so far.\\

\noindent \textbf{Insert} - The insert method takes a key as an input. The key can be in various forms of data types. For simplicity, let's take an integer as the input. The heap priority, which is also an integer, will be created from the input in a deterministic manner. We can deploy a hash function to perform this operation. Once the method populates a key and a priority, it can start traversing the node to find an appropriate position. A rotation might perform to restore heap property. The average complexity of this operation is $O(\log n)$, but it can be as bad as $O(n)$ if the tree forms a linked-list. Therefore, picking a good hash function for heap property is essential to avoid forming an unbalanced tree.\\

\noindent \textbf{Build} - The build method takes an array of keys as an input. It basically performs insertions for multiple keys in a row. Therefore, we can simply call insert method for the build implementation. Depending on how the tree is structured, it can take a linear time $O(n)$ if the input array is already sorted. However, sorting an array can take $O(n \log n)$ times assuming we are using merge sort. In this project, we will not limit the input array such that it can be in a random order and has duplicates. Hence, the overall time complexity would be $O(m \log n)$ where $m$ is the array length and $n$ is the total available nodes.\\

\noindent \textbf{Delete} - The delete method takes a key as an input to find the corresponding node. We can only delete a node if it is a leaf or one of its children is $null$. Therefore, some rotations perform to bring the node down to the leaf which can take an average time of $O(\log n)$.\\

\noindent \textbf{Search} - Using an input key, the search operation is quite straightforward as we already use it during the insert and the delete operation. It takes an average of $O(\log n)$ times to find the node.\\

\noindent \textbf{Split} - The split operation divides a treap into two treaps using an input key such that the left treap keys are at most the input key, and the right treap keys are greater than the input key. To implement split, we first search it. Either found or not found, we can simply disconnect the relation of the parent node and its right child. Since the right child's left child has lesser key (if not $null$), we can promote it to become the right child of the current parent node. The averate time complexity to perform this operation is $O(\log n)$.\\

\noindent \textbf{Merge} - The merge operation takes two treaps to form one treap. The operation is actually reverting two trees back to one after splitted. Having said that, it is assumed that the input treaps are well-ordered such that all keys in the left treap are smaller than all keys in the right treap. If two treaps do not have the ordered property, we can use $union$ operation to put them together into a single treap, but we will not discuss $union$ in this report. To perform merge operation, we split the right treap using the root's key from the left treap to get a new left and right treap. We then merge them with the corresponding left and right child of the root of the left treap. We keep doing these activities recursively until it gets into leaves. The averate time complexity to perform this operation is $O(\log n)$.\\

\subsection{Treap Applications}
Treaps have been used in various scenarios due to their simplicity and strong average-case performance. Among the frequent use cases for treaps are dynamic ordered sets, range queries, priority queue, and many more. Treaps has the capacity to keep a dynamic set of ordered elements, making search, insertion, and deletion operations effective. This qualifies them for uses like keeping a sorted list of users, things, or events, which involve frequent modifications to the data collection. \\

There are some research regarding the application of treaps. Blelloch and Margaret proposed treaps to build fast set operations such as union, intersection, and difference under $O(m \log(n/m))$ time complexity \cite{blelloch1998fast}. For memory appliction, since memory nowadays is getting large, treap is used in memory indexing to better interact with caches. Although memory performance is considered fast, it is still slow compared with on-chip caches. Another interesting application was also suggested by Dharya and Shalini, who proposed using treaps instead of adjacency matrix to optimize graph storage. \cite{arora2012using}. Based on those applications, we decided to formalize treaps as a part of this project. In the following sections, we will go through the details on how we formalized and verified some operations in treaps.\\

